{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import pymongo\n",
    "from urllib.parse import quote_plus as quote\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "collection_name = 'augmented_dataset' #['unique_dataset', 'augmented_dataset', 'initial_dataset']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Подключимся к нашей коллекции\n",
    "\n",
    "with open('../env/credsw.yaml', 'r') as file:\n",
    "    creds_dict = yaml.safe_load(file)\n",
    "    \n",
    "url = 'mongodb://{user}:{pw}@{hosts}/?{rs}&authSource={auth_src}&{am}&tls=true&tlsCAFile={cert_file}'.format(\n",
    "    user=creds_dict['username'],\n",
    "    pw=quote(creds_dict['password']),\n",
    "    hosts=creds_dict['host'],\n",
    "    rs='replicaSet=rs01',\n",
    "    auth_src=creds_dict['database'],\n",
    "    am='authMechanism=DEFAULT',\n",
    "    cert_file='../env/root.crt'\n",
    "    )\n",
    "\n",
    "dbs = pymongo.MongoClient(url)\n",
    "\n",
    "db = dbs[creds_dict['database']]\n",
    "\n",
    "collection = db[collection_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выгрузим данные из коллекции\n",
    "pipeline = [{ \"$unwind\" : \"$boxes\" },\n",
    "            {\"$project\":{\n",
    "                \"size\": \"$boxes.size\",\n",
    "                \"stacking\": \"$boxes.stacking\",\n",
    "                \"turnover\": \"$boxes.turnover\",\n",
    "                \"loading_size\": \"$loading_size\",\n",
    "                \"density_percent\": \"$density_percent\"\n",
    "                }},\n",
    "            {\"$group\": {\"_id\": \"$_id\",\n",
    "                        \"loading_size\": {\"$first\":\"$loading_size\"},\n",
    "                        \"density_percent\": {\"$first\":\"$density_percent\"},\n",
    "                        \"boxes\": {\n",
    "                            \"$push\":  {\n",
    "                                \"size\": \"$size\",\n",
    "                                \"stacking\": \"$stacking\",\n",
    "                                \"turnover\": \"$turnover\"                         \n",
    "                            }\n",
    "                            \n",
    "                        }\n",
    "            }\n",
    "            }\n",
    "            ]\n",
    "result = collection.aggregate(pipeline, allowDiskUse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Подготовим train и test datasets\n",
    "test_size = 1000\n",
    "\n",
    "dataset = [\n",
    "    (item['density_percent'],\n",
    "    [[item['loading_size']['width'], item['loading_size']['height'], item['loading_size']['length'], False, False]] + \\\n",
    "    [ [box['size']['width'], box['size']['height'], box['size']['length'], box['stacking'], box['turnover']] for box in item['boxes']])\n",
    "    for item in result\n",
    "]\n",
    "\n",
    "#y_train, y_test = [round(item[0],0) for item in dataset[0:-test_size]], [round(item[0],0) for item in dataset[-test_size : ]]\n",
    "y_train, y_test = [[1 if i==round(item[0],0) else 0 for i in range(101)] for item in dataset[0:-test_size]], [[1 if i==round(item[0],0) else 0 for i in range(101)] for item in dataset[-test_size : ]]\n",
    "X_train, X_test = [item[1] for item in dataset[0:-test_size]], [item[1] for item in dataset[-test_size:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выровняем размерности входных матриц X\n",
    "max_size = max(max([len(i) for i in X_train]), max([len(i) for i in X_test]))\n",
    "\n",
    "for i in range(len(X_train)):\n",
    "    add_dims = max_size - len(X_train[i])\n",
    "    X_train[i] = np.concatenate((np.array(X_train[i]), np.zeros((add_dims, 5))))\n",
    "\n",
    "for i in range(len(X_test)):\n",
    "    add_dims = max_size - len(X_test[i])\n",
    "    X_test[i] = np.concatenate((np.array(X_test[i]), np.zeros((add_dims, 5))))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d (Conv1D)             (None, 2850, 16)          176       \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, 1425, 16)         0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 1425, 16)          528       \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPooling  (None, 712, 16)          0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 712, 16)           528       \n",
      "                                                                 \n",
      " max_pooling1d_2 (MaxPooling  (None, 356, 16)          0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 356, 16)           0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 5696)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 101)               575397    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 576,629\n",
      "Trainable params: 576,629\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv1D(filters=16, kernel_size=2, padding='same', activation='relu', input_shape=(max_size, 5)))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "model.add(Conv1D(filters=16, kernel_size=2, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "model.add(Conv1D(filters=16, kernel_size=2, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(101, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 1.53086, saving model to model.weights.best.hdf5\n",
      "175/175 - 10s - loss: 1.8965 - accuracy: 0.4089 - val_loss: 1.5309 - val_accuracy: 0.4730 - 10s/epoch - 58ms/step\n",
      "Epoch 2/10\n",
      "\n",
      "Epoch 2: val_loss improved from 1.53086 to 0.97285, saving model to model.weights.best.hdf5\n",
      "175/175 - 9s - loss: 1.2017 - accuracy: 0.6531 - val_loss: 0.9729 - val_accuracy: 0.7500 - 9s/epoch - 51ms/step\n",
      "Epoch 3/10\n",
      "\n",
      "Epoch 3: val_loss improved from 0.97285 to 0.80375, saving model to model.weights.best.hdf5\n",
      "175/175 - 9s - loss: 0.8414 - accuracy: 0.7869 - val_loss: 0.8037 - val_accuracy: 0.7810 - 9s/epoch - 51ms/step\n",
      "Epoch 4/10\n",
      "\n",
      "Epoch 4: val_loss improved from 0.80375 to 0.58443, saving model to model.weights.best.hdf5\n",
      "175/175 - 9s - loss: 0.6623 - accuracy: 0.8306 - val_loss: 0.5844 - val_accuracy: 0.8620 - 9s/epoch - 51ms/step\n",
      "Epoch 5/10\n",
      "\n",
      "Epoch 5: val_loss improved from 0.58443 to 0.51151, saving model to model.weights.best.hdf5\n",
      "175/175 - 9s - loss: 0.5864 - accuracy: 0.8453 - val_loss: 0.5115 - val_accuracy: 0.8640 - 9s/epoch - 51ms/step\n",
      "Epoch 6/10\n",
      "\n",
      "Epoch 6: val_loss improved from 0.51151 to 0.47253, saving model to model.weights.best.hdf5\n",
      "175/175 - 9s - loss: 0.5318 - accuracy: 0.8525 - val_loss: 0.4725 - val_accuracy: 0.8980 - 9s/epoch - 51ms/step\n",
      "Epoch 7/10\n",
      "\n",
      "Epoch 7: val_loss improved from 0.47253 to 0.43528, saving model to model.weights.best.hdf5\n",
      "175/175 - 9s - loss: 0.4893 - accuracy: 0.8695 - val_loss: 0.4353 - val_accuracy: 0.8920 - 9s/epoch - 49ms/step\n",
      "Epoch 8/10\n",
      "\n",
      "Epoch 8: val_loss improved from 0.43528 to 0.43234, saving model to model.weights.best.hdf5\n",
      "175/175 - 9s - loss: 0.4557 - accuracy: 0.8799 - val_loss: 0.4323 - val_accuracy: 0.8930 - 9s/epoch - 51ms/step\n",
      "Epoch 9/10\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.43234\n",
      "175/175 - 9s - loss: 0.4212 - accuracy: 0.8939 - val_loss: 0.4912 - val_accuracy: 0.8940 - 9s/epoch - 50ms/step\n",
      "Epoch 10/10\n",
      "\n",
      "Epoch 10: val_loss improved from 0.43234 to 0.36420, saving model to model.weights.best.hdf5\n",
      "175/175 - 9s - loss: 0.3877 - accuracy: 0.9037 - val_loss: 0.3642 - val_accuracy: 0.9330 - 9s/epoch - 50ms/step\n"
     ]
    }
   ],
   "source": [
    "checkpointer = ModelCheckpoint(filepath='model.weights.best.hdf5', verbose=1, save_best_only=True)\n",
    "hist = model.fit(np.array(X_train),np.array(y_train), batch_size=50, epochs=10, validation_data=(np.array(X_test), np.array(y_test)),callbacks=[checkpointer], verbose=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(np.array(X_test), np.array(y_test), verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.9330000281333923\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test accuracy: {score[1]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Проверка на нескольких случайных примерах из test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 83; Actual: 83; Guessed: True\n",
      "Prediction: 100; Actual: 100; Guessed: True\n",
      "Prediction: 100; Actual: 100; Guessed: True\n",
      "Prediction: 86; Actual: 86; Guessed: True\n",
      "Prediction: 100; Actual: 100; Guessed: True\n",
      "Prediction: 74; Actual: 74; Guessed: True\n",
      "Prediction: 100; Actual: 100; Guessed: True\n",
      "Prediction: 70; Actual: 70; Guessed: True\n",
      "Prediction: 74; Actual: 74; Guessed: True\n",
      "Prediction: 100; Actual: 100; Guessed: True\n",
      "Prediction: 100; Actual: 100; Guessed: True\n",
      "Prediction: 83; Actual: 83; Guessed: True\n",
      "Prediction: 70; Actual: 70; Guessed: True\n",
      "Prediction: 100; Actual: 100; Guessed: True\n",
      "Prediction: 70; Actual: 70; Guessed: True\n"
     ]
    }
   ],
   "source": [
    "for i in random.sample(range(1000), 15):\n",
    "    prediction = model.predict(np.array([X_test[i]]), verbose=0)\n",
    "    prediction = np.where(prediction[0]==max(prediction[0]))[0][0]\n",
    "    actual = y_test[i].index(1)\n",
    "    print(f\"Prediction: {prediction}; Actual: {actual}; Guessed: {actual==prediction}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
